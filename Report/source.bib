
@InProceedings{Xia_2021_CVPR,
    author    = {Xia, Weihao and Yang, Yujiu and Xue, Jing-Hao and Wu, Baoyuan},
    title     = {TediGAN: Text-Guided Diverse Face Image Generation and Manipulation},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {2256-2265}
}

@inproceedings{NEURIPS2019_0234c510,
 author = {Kynk\"{a}\"{a}nniemi, Tuomas and Karras, Tero and Laine, Samuli and Lehtinen, Jaakko and Aila, Timo},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Improved Precision and Recall Metric for Assessing Generative Models},
 url = {https://proceedings.neurips.cc/paper/2019/file/0234c510bc6d908b28c70ff313743079-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{NIPS2017_8a1d6947,
 author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium},
 url = {https://proceedings.neurips.cc/paper/2017/file/8a1d694707eb0fefe65871369074926d-Paper.pdf},
 volume = {30},
 year = {2017}
}

@ARTICLE{Tang,
  author={Tang, Hao and Chen, Xinya and Wang, Wei and Xu, Dan and Corso, Jason J. and Sebe, Nicu and Yan, Yan},
  booktitle={2019 14th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2019)}, 
  title={Attribute-Guided Sketch Generation}, 
  year={2019},
  volume={4},
  pages={1-7},
  doi={10.1109/FG.2019.8756586}
}

@ARTICLE{6359918,
  author={Han, Hu and Klare, Brendan F. and Bonnen, Kathryn and Jain, Anil K.},
  journal={IEEE Transactions on Information Forensics and Security}, 
  title={Matching Composite Sketches to Face Photos: A Component-Based Approach}, 
  year={2013},
  volume={8},
  number={1},
  pages={191-204},
  doi={10.1109/TIFS.2012.2228856}
}

@article{LEI202013,
title = {Face sketch-to-photo transformation with multi-scale self-attention GAN},
journal = {Neurocomputing},
volume = {396},
pages = {13-23},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.02.024},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220301995},
author = {Yingtao Lei and Weiwei Du and Qinghua Hu},
keywords = {Image transformation, Sketch-to-photo, Divide and conquer, Multi-scale, Attention mechanism, Generative adversarial network},
abstract = {In this study, we investigate the sketch-to-photo problem, which currently poses a significant challenge in the field of computer vision. A large number of GAN-based encoder-decoder methods have been proposed for image transformation, inspired by the pix2pix model; however, these methods do not produce satisfactory results for photo generation, due to the fact that (1) they miss detailed information of input images because of a single-scale convolution operator in the shallow encoder layers, and (2) they fail to learn long-range dependencies in the deep encoder layers. To better handle these challenges, we present an approach that follows a “divide and conquer” strategy. Our method combines the advantages of a multi-scale convolutional neural network and an attention mechanism and applies these two modules to different encoder layers. Additionally, by optimizing a well-designed loss function, the complex correlations between the sketch and the photo can be calculated. Experimental results show that our method is able to generate high-quality photos from sketch images, and qualitative and quantitative analysis demonstrates its effectiveness and superiority over state-of-the-art models. This work paves a path to replace the traditional encoder structure with the “divide and conquer” strategy to handle image transformation tasks.}
}

@article{SANNIDHAN2019452,
title = {Evaluating the performance of face sketch generation using generative adversarial networks},
journal = {Pattern Recognition Letters},
volume = {128},
pages = {452-458},
year = {2019},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2019.10.010},
url = {https://www.sciencedirect.com/science/article/pii/S0167865519302831},
author = {M.S. Sannidhan and G. {Ananth Prabhu} and David E. Robbins and Charles Shasky},
keywords = {Adversarial network, Convolution neural network, Deep learning, Sketch to photo matching, Sketch generation, Photo generation},
abstract = {ABSTRACT
One of the most significant and widely used methods for identifying a culprit in the field of forensic science is generating a sketch of the suspect from descriptions given by an eyewitness to the crime. However, there is a high level of uncertainty in recognizing an individual solely from a sketch. Recognition of sketches based on photos of an individual is non-trivial, as there are differences between the domain features of a sketch and a photo. To streamline this process, this article presents a methodology for generating a colored photo from a sketch, which can then be used for identification using a variety of classification techniques. Implementation of the proposed method involves a trained Convolution Neural Network for sketch generation paired with a conditional Generative Adversarial Network's pix2pix model for color photo generation. Experimental results of the work are validated using standard datasets, and the proposed model achieved a minimum average rate of 65% similarity index value on all employed datasets with a training efficiency of more than 98% in every epoch level.}
}

@InProceedings{Lu_2018_ECCV,
author = {Lu, Yongyi and Tai, Yu-Wing and Tang, Chi-Keung},
title = {Attribute-Guided Face Generation Using Conditional CycleGAN},
booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
month = {September},
year = {2018}
}

@ARTICLE{9117185,
  author={Otberdout, Naima and Daoudi, Mohamed and Kacem, Anis and Ballihi, Lahoucine and Berretti, Stefano},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Dynamic Facial Expression Generation on Hilbert Hypersphere With Conditional Wasserstein Generative Adversarial Nets}, 
  year={2022},
  volume={44},
  number={2},
  pages={848-863},
  doi={10.1109/TPAMI.2020.3002500}
}

@INPROCEEDINGS{9320290,
  author={Dhar, Prithviraj and Bansal, Ankan and Castillo, Carlos D. and Gleason, Joshua and Phillips, P. Jonathon and Chellappa, Rama},
  booktitle={2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)}, 
  title={How are attributes expressed in face DCNNs?}, 
  year={2020},
  volume={},
  number={},
  pages={85-92},
  doi={10.1109/FG47880.2020.00009}
}

@INPROCEEDINGS{9412022,
  author={Yuan, Zheng and Zhang, Jie and Shan, Shiguang and Chen, Xilin},
  booktitle={2020 25th International Conference on Pattern Recognition (ICPR)}, 
  title={Attributes Aware Face Generation with Generative Adversarial Networks}, 
  year={2021},
  volume={},
  number={},
  pages={1657-1664},
  doi={10.1109/ICPR48806.2021.9412022}
}

@INPROCEEDINGS{8462648,
  author={Chen, Zhu-Liang and He, Qian-Hua and Pang, Wen-Feng and Li, Yan-Xiong},
  booktitle={2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Frontal Face Generation from Multiple Pose-Variant Faces with CGAN in Real-World Surveillance Scene}, 
  year={2018},
  volume={},
  number={},
  pages={1308-1312},
  doi={10.1109/ICASSP.2018.8462648}}

@InProceedings{Gecer_2018_ECCV,
author = {Gecer, Baris and Bhattarai, Binod and Kittler, Josef and Kim, Tae-Kyun},
title = {Semi-supervised Adversarial Learning to Generate Photorealistic Face Images of New Identities from 3D Morphable Model},
booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
month = {September},
year = {2018}
}

@ARTICLE{8606936,
  author={Deng, Jia and Pang, Gaoyang and Zhang, Zhiyu and Pang, Zhibo and Yang, Huayong and Yang, Geng},
  journal={IEEE Access}, 
  title={cGAN Based Facial Expression Recognition for Human-Robot Interaction}, 
  year={2019},
  volume={7},
  number={},
  pages={9848-9859},
  doi={10.1109/ACCESS.2019.2891668}}

@ARTICLE{9146375,
  author={Liu, Yaoyao and Sun, Qianru and He, Xiangnan and Liu, An-An and Su, Yuting and Chua, Tat-Seng},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Generating Face Images With Attributes for Free}, 
  year={2021},
  volume={32},
  number={6},
  pages={2733-2743},
  doi={10.1109/TNNLS.2020.3007790}}

@InProceedings{Bodla_2018_ECCV,
author = {Bodla, Navaneeth and Hua, Gang and Chellappa, Rama},
title = {Semi-supervised FusedGAN for Conditional Image Generation},
booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
month = {September},
year = {2018}
}

@inproceedings{NIPS2016_b1301141,
 author = {van den Oord, Aaron and Kalchbrenner, Nal and Espeholt, Lasse and kavukcuoglu, koray and Vinyals, Oriol and Graves, Alex},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Conditional Image Generation with PixelCNN Decoders},
 url = {https://proceedings.neurips.cc/paper/2016/file/b1301141feffabac455e1f90a7de2054-Paper.pdf},
 volume = {29},
 year = {2016}
}

@article{gauthier2014conditional,
  title={Conditional generative adversarial nets for convolutional face generation},
  author={Gauthier, Jon},
  journal={Class project for Stanford CS231N: convolutional neural networks for visual recognition, Winter semester},
  volume={2014},
  number={5},
  pages={2},
  year={2014}
}

@InProceedings{Xiao_2018_ECCV,
author = {Xiao, Taihong and Hong, Jiapeng and Ma, Jinwen},
title = {ELEGANT: Exchanging Latent Encodings with GAN for Transferring Multiple Face Attributes},
booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
month = {September},
year = {2018}
}

@article{DBLP,
  author    = {Richard Zhang and
               Phillip Isola and
               Alexei A. Efros and
               Eli Shechtman and
               Oliver Wang},
  title     = {The Unreasonable Effectiveness of Deep Features as a Perceptual Metric},
  journal   = {CoRR},
  volume    = {abs/1801.03924},
  year      = {2018},
  url       = {http://arxiv.org/abs/1801.03924},
  eprinttype = {arXiv},
  eprint    = {1801.03924},
  timestamp = {Wed, 14 Aug 2019 08:23:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1801-03924.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{6272356,
  author={Mittal, Anish and Moorthy, Anush Krishna and Bovik, Alan Conrad},
  journal={IEEE Transactions on Image Processing}, 
  title={No-Reference Image Quality Assessment in the Spatial Domain}, 
  year={2012},
  volume={21},
  number={12},
  pages={4695-4708},
  doi={10.1109/TIP.2012.2214050}}

@misc { photofitDef,
    author = {Cambridge dictionary},
    title = {photofit (picture)},
    howpublished = {\url{https://dictionary.cambridge.org/de/worterbuch/englisch/photofit-picture}},
    note = {Accessed: 2021-08-11}
}

@misc { dallE,
    author = {openai},
    title = {Dall-E},
    howpublished = {\url{https://openai.com/dall-e-2/}},
    note = {Accessed: 2021-08-11}
}

@inproceedings{celebA,
  title = {Deep Learning Face Attributes in the Wild},
  author = {Liu, Ziwei and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},
  booktitle = {Proceedings of International Conference on Computer Vision (ICCV)},
  month = {December},
  year = {2015} 
}

@inproceedings{CelebAMask-HQ,
  title = {MaskGAN: Towards Diverse and Interactive Facial Image Manipulation},
  author = {Lee, Cheng-Han and Liu, Ziwei and Wu, Lingyun and Luo, Ping},
  booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year = {2020}
}

@misc{CelebAMask-HQ,
  doi = {10.48550/ARXIV.1710.10196},
  url = {https://arxiv.org/abs/1710.10196},
  author = {Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
  keywords = {Neural and Evolutionary Computing (cs.NE), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Progressive Growing of GANs for Improved Quality, Stability, and Variation},
  publisher = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@TechReport{LFW,
  author =       {Gary B. Huang and Manu Ramesh and Tamara Berg and 
                  Erik Learned-Miller},
  title =        {Labeled Faces in the Wild: A Database for Studying 
                  Face Recognition in Unconstrained Environments},
  institution =  {University of Massachusetts, Amherst},
  year =         2007,
  number =       {07-49},
  month =        {October}}

@article{DBLP2,
  author    = {Philipp Terhörst and
               Daniel Fährmann and
               Jan Niklas Kolf and
               Naser Damer and
               Florian Kirchbuchner and
               Arjan Kuijper},
  title     = {MAAD-Face: {A} Massively Annotated Attribute Dataset for Face Images},
  journal   = {CoRR},
  volume    = {abs/2012.01030},
  year      = {2020},
  url       = {https://arxiv.org/abs/2012.01030},
  eprinttype = {arXiv},
  eprint    = {2012.01030},
  timestamp = {Thu, 14 Oct 2021 09:16:32 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2012-01030.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{gan,
  doi = {10.48550/ARXIV.1406.2661},
  url = {https://arxiv.org/abs/1406.2661},
  author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Generative Adversarial Networks},
  publisher = {arXiv},
  year = {2014},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{dcgan,
  doi = {10.48550/ARXIV.1511.06434},
  url = {https://arxiv.org/abs/1511.06434},
  author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks},
  publisher = {arXiv},
  year = {2015},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
